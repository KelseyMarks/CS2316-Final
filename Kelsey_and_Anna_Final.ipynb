{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KelseyMarks/CS2316-Final/blob/main/Kelsey_and_Anna_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Kelsey and Anna Phase II#\n",
        "This is the data cleaning phase of the CS2316 Final Project.  For our project, we are looking at crime rates and how they compare to drug usage, education level, population density, and more in Atlanta, Boston, New York City, and Los Angeles.\n",
        "\n",
        "The first thing we will do is import the needed tools for cleaning and gathering all of our data."
      ],
      "metadata": {
        "id": "Tjq75A8hMc9D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZ2UAyj3zNbU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset 1 - The Downloaded Dataset#\n",
        "\n",
        "For this dataset, we are cleaning a CSV file that contains the Atlanta Police Department Crime Log.  Cleaning this dataset will include removing unnecessary columns, adding an additional column for whether or not the crime was violent, and renaming the columns to be uniform with the other data sets we will be using."
      ],
      "metadata": {
        "id": "UKYUgnIBMwzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_parser():\n",
        "  df = pd.read_csv(\"COBRA-2009-2019.csv\")\n",
        "  df = df[['Occur Date', 'Occur Time', 'UCR Literal']]\n",
        "  df[['Year', 'Month', 'Day']] = df['Occur Date'].str.split('-', expand=True)\n",
        "  df = df.drop(['Month', 'Day'], axis=1)\n",
        "  df = df[df['Year'] == '2019']\n",
        "  df = df.drop('Occur Date', axis=1)\n",
        "  #print(df['UCR Literal'].value_counts())\n",
        "  df['Violent'] = 'No'\n",
        "  df.loc[df['UCR Literal']=='BURGLARY-RESIDENCE', 'Violent'] = 'Yes'\n",
        "  df.loc[df['UCR Literal']=='AGG ASSAULT', 'Violent'] = 'Yes'\n",
        "  df.loc[df['UCR Literal']=='BURGLARY-NONERES', 'Violent'] = 'Yes'\n",
        "  df.loc[df['UCR Literal']=='HOMICIDE', 'Violent'] = 'Yes'\n",
        "  df.loc[df['UCR Literal']=='MANSLAUGHTER', 'Violent'] = 'Yes'\n",
        "  df.rename(columns={'Occur Time': 'Time', 'UCR Literal': 'Crime'}, inplace=True)\n",
        "  print(df)\n",
        "  df.to_csv('ATL Crime Cleaned.csv', index=False)\n",
        "\n",
        "############ Function Call ############\n",
        "data_parser()"
      ],
      "metadata": {
        "id": "EJmI_RcYzfh1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a8cb15d-fb8f-405a-9523-f75bd6317dd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: DtypeWarning: Columns (3,11) have mixed types.Specify dtype option on import or set low_memory=False.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Time                 Crime  Year Violent\n",
            "317884  0100  LARCENY-FROM VEHICLE  2019      No\n",
            "317890  0020   LARCENY-NON VEHICLE  2019      No\n",
            "317891  0120   LARCENY-NON VEHICLE  2019      No\n",
            "317892  1740   LARCENY-NON VEHICLE  2019      No\n",
            "317895  0400  LARCENY-FROM VEHICLE  2019      No\n",
            "...      ...                   ...   ...     ...\n",
            "342909  2030           AGG ASSAULT  2019     Yes\n",
            "342910   432           AGG ASSAULT  2019     Yes\n",
            "342911   920           AGG ASSAULT  2019     Yes\n",
            "342912  1853           AGG ASSAULT  2019     Yes\n",
            "342913  2045           AGG ASSAULT  2019     Yes\n",
            "\n",
            "[24853 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset 2 - Web Collection Requirement 1#\n",
        "\n",
        "For this dataset, we wanted to get the personal income for each city of interest, New York City, Boston, Atlanta, and Los Angeles. We removed all other cities, as well as all other columns containing extra financial data about the cities."
      ],
      "metadata": {
        "id": "GJ-TqoglNko7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def web_parser1():\n",
        "  final_list=[]\n",
        "  final_dict={}\n",
        "  response=requests.get(\"https://en.wikipedia.org/wiki/List_of_U.S._cities_by_adjusted_per_capita_personal_income\")\n",
        "  soup=BeautifulSoup(response.text,\"html.parser\")\n",
        "  table_tag=soup.find(\"table\")\n",
        "  tr_tag=table_tag.find_all(\"tr\")\n",
        "\n",
        "  for tr in tr_tag:\n",
        "    final_list.append(tr.text.strip())\n",
        "\n",
        "  for city in final_list:\n",
        "    city=city.split(\"\\n\")\n",
        "    final_dict[city[0]]=city[1:]\n",
        "\n",
        "  value_dict={}\n",
        "  for key,value in final_dict.items():\n",
        "    value_dict[key]=value[0]  \n",
        "\n",
        "  value_dict[\"New York City\"]=value_dict[\"1.  New York City-Newark-Jersey City, NY-NJ-PA MSA\"]\n",
        "  value_dict[\"Los Angeles\"]=value_dict[\"2.  Los Angeles-Long Beach-Anaheim, CA MSA\"]\n",
        "  value_dict[\"Atlanta\"]=value_dict[\"9.  Atlanta-Sandy Springs-Alpharetta, GA MSA\"]\n",
        "  value_dict[\"Boston\"]=value_dict[\"11. Boston-Worcester-Providence, MA-RI-NH-CT CSA\"]\n",
        "  # print(value_dict)\n",
        "  final_dict={}\n",
        "  for key,value in value_dict.items():\n",
        "    if key==\"Boston\" or key==\"New York City\" or key==\"Atlanta\" or key==\"Los Angeles\":\n",
        "      final_dict[key]=value\n",
        "  print(final_dict)\n",
        "  f = open('CleanedIncome.txt', \"w\")\n",
        "  f.write(str(final_dict))\n",
        "  f.close()\n",
        "\n",
        "############ Function Call ############\n",
        "web_parser1()"
      ],
      "metadata": {
        "id": "bHNlE6qcz0P3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48e91f8e-387e-4ec9-ada6-f60aca3688af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'New York City': '$79,844', 'Los Angeles': '$66,684', 'Atlanta': '$54,557', 'Boston': '$81,498'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inconsistency 1#\n",
        "In our first web collection requirement, we are looking at the average personal per capita income for the cities we are interested in. The main inconsistency looking at this data was that the city names were formatted differently than other datasets we are looking that. To fix this, we first used Beautiful Soup to get the html table from the website. We then went in and changed the dictionary of all the cities and their per capita incomes, to just include the ones we want. Finally, we changed the names to just be the city name itself to better match the formatting of the other datasets."
      ],
      "metadata": {
        "id": "NsSb_q5gOMZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset 3 - Web Collection Requirement 2#\n",
        "\n",
        "For this dataset, we accessed the FBI's API for their most wanted list in order to see the hometowns of those on the FBI's Most Wanted list.  To do this, we went through each page of the api and added the hometowns to a list."
      ],
      "metadata": {
        "id": "EP5zQKjuOkyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def web_parser2():\n",
        "  hometowns = []\n",
        "  hometowns_us = []\n",
        "  hometowns_clean = []\n",
        "  for n in range(1, 50):  # 50 pages\n",
        "      response = requests.get(\n",
        "          \"https://api.fbi.gov/wanted/v1/list\", params={\"page\": n}\n",
        "      )\n",
        "      data = json.loads(response.content)\n",
        "      for i in range(1, len(data[\"items\"])):\n",
        "          if data[\"items\"][i][\"place_of_birth\"] != None:\n",
        "              hometowns.append(data[\"items\"][i][\"place_of_birth\"])\n",
        "  state_names = [\n",
        "      \"Alaska\",\n",
        "      \"Alabama\",\n",
        "      \"Arkansas\",\n",
        "      \"American Samoa\",\n",
        "      \"Arizona\",\n",
        "      \"California\",\n",
        "      \"Colorado\",\n",
        "      \"Connecticut\",\n",
        "      \"District \",\n",
        "      \"of Columbia\",\n",
        "      \"Delaware\",\n",
        "      \"Florida\",\n",
        "      \"Georgia\",\n",
        "      \"Guam\",\n",
        "      \"Hawaii\",\n",
        "      \"Iowa\",\n",
        "      \"Idaho\",\n",
        "      \"Illinois\",\n",
        "      \"Indiana\",\n",
        "      \"Kansas\",\n",
        "      \"Kentucky\",\n",
        "      \"Louisiana\",\n",
        "      \"Massachusetts\",\n",
        "      \"Maryland\",\n",
        "      \"Maine\",\n",
        "      \"Michigan\",\n",
        "      \"Minnesota\",\n",
        "      \"Missouri\",\n",
        "      \"Mississippi\",\n",
        "      \"Montana\",\n",
        "      \"North Carolina\",\n",
        "      \"North Dakota\",\n",
        "      \"Nebraska\",\n",
        "      \"New Hampshire\",\n",
        "      \"New Jersey\",\n",
        "      \"New Mexico\",\n",
        "      \"Nevada\",\n",
        "      \"New York\",\n",
        "      \"Ohio\",\n",
        "      \"Oklahoma\",\n",
        "      \"Oregon\",\n",
        "      \"Pennsylvania\",\n",
        "      \"Puerto Rico\",\n",
        "      \"Rhode Island\",\n",
        "      \"South Carolina\",\n",
        "      \"South Dakota\",\n",
        "      \"Tennessee\",\n",
        "      \"Texas\",\n",
        "      \"Utah\",\n",
        "      \"Virginia\",\n",
        "      \"Virgin Islands\",\n",
        "      \"Vermont\",\n",
        "      \"Washington\",\n",
        "      \"Wisconsin\",\n",
        "      \"West Virginia\",\n",
        "      \"Wyoming\",\n",
        "  ]\n",
        "  for state in state_names:\n",
        "      for place in hometowns:\n",
        "          if state in place:\n",
        "              hometowns_us.append(place)\n",
        "  for place in hometowns_us:\n",
        "      if len(place.split(\",\")) > 1:\n",
        "          hometowns_clean.append(place.split(\",\")[1][1:])\n",
        "      else:\n",
        "          hometowns_clean.append(place.split(\",\")[0])\n",
        "  hometowns_clean.remove(\"Gloucester County\")\n",
        "  hometowns_clean.remove(\"Washington State\")\n",
        "  hometowns_clean.remove(\"USA\")\n",
        "  hometowns_clean.remove(\"Queens\")\n",
        "\n",
        "  print(hometowns_clean)\n",
        "  f = open('apiHometownsList.txt', \"w\")\n",
        "  f.write(str(hometowns_clean))\n",
        "  f.close()\n",
        "\n",
        "############ Function Call ############\n",
        "web_parser2()"
      ],
      "metadata": {
        "id": "w6fejiPTNlKw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24cc1f60-0401-43b7-eb6e-9d53082423af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Alabama', 'Alabama', 'Arkansas', 'Arizona', 'Arizona', 'Arizona', 'Arizona', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'California', 'Colorado', 'Colorado', 'Connecticut', 'Connecticut', 'Delaware', 'Florida', 'Florida', 'Florida', 'Florida', 'Florida', 'Florida', 'Georgia', 'Georgia', 'Georgia', 'Hawaii', 'Idaho', 'Idaho', 'Illinois', 'Illinois', 'Illinois', 'Illinois', 'Illinois', 'United States of America', 'Illinois', 'Illinois', 'Illinois', 'Indiana', 'Indiana', 'Indiana', 'Kansas', 'Missouri', 'Missouri', 'Louisiana', 'Massachusetts', 'Massachusetts', 'Maryland', 'Michigan', 'Michigan', 'Michigan', 'Michigan', 'Michigan', 'Michigan', 'Michigan', 'Michigan', 'Minnesota', 'Missouri', 'Missouri', 'Missouri', 'Missouri', 'Missouri', 'North Carolina', 'North Carolina', 'North Carolina', 'North Dakota', 'Nebraska', 'Nebraska', 'New Hampshire', 'New Hampshire', 'New Jersey', 'New Jersey', 'New Jersey', 'New Jersey', 'New Jersey', 'New Jersey', 'New Jersey', 'New Mexico', 'New Mexico', 'New Mexico', 'New Mexico', 'New York', 'New York', 'New York', 'New York', 'New York', 'New York', 'New York', 'New York', 'New York', 'New York', 'New York', 'New York', 'Ohio', 'Ohio', 'Ohio', 'Ohio', 'Oklahoma', 'Oregon', 'Oregon', 'Oregon', 'Oregon', 'Oregon', 'Oregon', 'Pennsylvania', 'Pennsylvania', 'Pennsylvania', 'Pennsylvania', 'Pennsylvania', 'Pennsylvania', 'Puerto Rico', 'Puerto Rico', 'Puerto Rico', 'South Dakota', 'South Dakota', 'South Dakota', 'Tennessee', 'Tennessee', 'Tennessee', 'Tennessee', 'Tennessee', 'Tennessee', 'Texas', 'Texas', 'Texas', 'Texas', 'Texas', 'Virginia', 'Virginia', 'Virginia', 'Virginia', 'Washington', 'Washington', 'Washington', 'Washington', 'DC', 'DC', 'Wisconsin', 'Wisconsin', 'Wisconsin', 'Wisconsin', 'Wyoming']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inconsistency 2#\n",
        "In our API source for the FBI's most wanted list, we took out all unknown hometowns and removed all places that are not in the US. We identified these issues by creating a list of the hometowns then looking through it to see what needed to be changed to fit our project. They were significant because we are focusing our project on the US and we need no unknown values for our models to be made in phase 3. We took out the unknown values by using a for loop going through the list and used a similar method to get rid of the places not in the US. Also, some of the places only included a state or country, or were listed as state, country, or city, state, so we made the formatting uniform to be just the state by splitting strings and only taking the state names.  After all of this, there were still a couple of values that did not fit the standard syntax we wanted for our data of just the state names so we individually removed those values. \n"
      ],
      "metadata": {
        "id": "31rgdtWkPBia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Additional Dataset 1#\n",
        "\n",
        "For this data, we webscraped a table from the FBI website about crimes in the state of New York.  We were specifically looking for numbers based on the types of crimes that occurred in New York City so that is the specific data we webscraped for."
      ],
      "metadata": {
        "id": "EEZJkWJXPlRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extra_source1():\n",
        "  resp = requests.get('https://ucr.fbi.gov/crime-in-the-u.s/2019/crime-in-the-u.s.-2019/tables/table-8/table-8-state-cuts/newyork.xls')\n",
        "  soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "  tags = soup.find_all('table')\n",
        "  data = {}\n",
        "  for tag in tags:\n",
        "      place = tag.find_all('tr')\n",
        "      for entry in place:\n",
        "          name = entry.find('th').text.strip()\n",
        "          if name == 'New York':\n",
        "              violent = entry.find_all('td')[1].text.strip()\n",
        "              murder = entry.find_all('td')[2].text.strip()\n",
        "              rape = entry.find_all('td')[3].text.strip()\n",
        "              robbery = entry.find_all('td')[4].text.strip()\n",
        "              assault = entry.find_all('td')[5].text.strip()\n",
        "              vandalism = entry.find_all('td')[6].text.strip()\n",
        "              burglary = entry.find_all('td')[7].text.strip()\n",
        "              larceny_theft = entry.find_all('td')[8].text.strip()\n",
        "              vehicle_theft = entry.find_all('td')[9].text.strip()\n",
        "              arson = entry.find_all('td')[10].text.strip()\n",
        "              data = {'NYC':name, 'Violent':violent, 'Murder':murder, 'Rape':rape, 'Robbery':robbery,\\\n",
        "              'Assault':assault, 'Vandalism':vandalism, 'Burglary':burglary, 'Larceny/Theft': larceny_theft,\\\n",
        "              'Vehicle Theft':vehicle_theft, 'Arson':arson}\n",
        "  print(data)\n",
        "  with open(\"NYC Data Cleaned.txt\", \"w\") as f:\n",
        "      f.write(str(data))\n",
        "\n",
        "############ Function Call ############\n",
        "extra_source1()"
      ],
      "metadata": {
        "id": "QS8mM0v4Nlt9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "156593a5-95b6-4e15-a52c-827f831a1892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'NYC': 'New York', 'Violent': '47,821', 'Murder': '319', 'Rape': '2,770', 'Robbery': '13,396', 'Assault': '31,336', 'Vandalism': '122,299', 'Burglary': '9,846', 'Larceny/Theft': '106,931', 'Vehicle Theft': '5,522', 'Arson': ''}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Additional Dataset 2#\n",
        "For this data, we webscraped a table from WalletHub to get a score for the level of education as well as the availability of education for the needed cities for our project.  This website unfortunately blocks your IP if you try to webscrape more than twice so we will have to pick out only the cities we want for our project from the cleaned data in Phase III when we use the cleaned list of all the cities we have downloaded."
      ],
      "metadata": {
        "id": "ATt2Nmo5Qspi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extra_source2():\n",
        "  resp = requests.get('https://wallethub.com/edu/e/most-and-least-educated-cities/6656')\n",
        "  soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "#  soup = BeautifulSoup(open(\"WalletHub.html\"), \"html.parser\")\n",
        "  tags = soup.find_all('tbody')\n",
        "  cities = {}\n",
        "  for tag in tags:\n",
        "      place = tag.find_all('tr')\n",
        "      for entry in place:\n",
        "          name = entry.find_all('td')[1].text\n",
        "          total_score = entry.find_all('td')[2].text\n",
        "          attain_edu = entry.find_all('td')[3].text\n",
        "          quality = entry.find_all('td')[4].text\n",
        "          cities[name] = [total_score, attain_edu, quality]\n",
        "  print(cities)\n",
        "\n",
        "######Commented out file writing in case this c\n",
        "  f = open('EducationLevels.txt', \"w\")\n",
        "  f.write(str(cities))\n",
        "  f.close()\n",
        "\n",
        "############ Function Call ############\n",
        "extra_source2()"
      ],
      "metadata": {
        "id": "qEElWnQ_QsGb",
        "outputId": "dbb537d7-5713-4748-e55c-b15f6fef12b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Ann Arbor, MI': ['93.99', '1', '1'], 'San Jose-Sunnyvale-Santa Clara, CA': ['82.03', '5', '5'], 'Washington-Arlington-Alexandria, DC-VA-MD-WV': ['81.78', '3', '27'], 'Madison, WI': ['80.83', '2', '49'], 'San Francisco-Oakland-Berkeley, CA': ['80.77', '4', '17'], 'Boston-Cambridge-Newton, MA-NH': ['78.26', '6', '47'], 'Durham-Chapel Hill, NC': ['78.06', '9', '4'], 'Raleigh-Cary, NC': ['77.08', '7', '44'], 'Seattle-Tacoma-Bellevue, WA': ['75.34', '10', '14'], 'Austin-Round Rock-Georgetown, TX': ['73.84', '11', '13'], 'Bridgeport-Stamford-Norwalk, CT': ['71.96', '8', '146'], 'Provo-Orem, UT': ['71.73', '17', '24'], 'Colorado Springs, CO': ['71.64', '14', '30'], 'Denver-Aurora-Lakewood, CO': ['70.78', '12', '79'], 'Trenton-Princeton, NJ': ['70.11', '15', '71'], 'Portland-South Portland, ME': ['69.58', '16', '70'], 'Portland-Vancouver-Hillsboro, OR-WA': ['69.24', '19', '50'], 'Tallahassee, FL': ['68.47', '22', '7'], 'Minneapolis-St. Paul-Bloomington, MN-WI': ['68.12', '13', '116'], 'Albany-Schenectady-Troy, NY': ['67.58', '20', '83'], 'Huntsville, AL': ['67.41', '24', '12'], 'San Diego-Chula Vista-Carlsbad, CA': ['67.13', '25', '3'], 'Baltimore-Columbia-Towson, MD': ['67.07', '18', '118'], 'Lexington-Fayette, KY': ['66.17', '23', '31'], 'New York-Newark-Jersey City, NY-NJ-PA': ['65.23', '27', '16'], 'Atlanta-Sandy Springs-Alpharetta, GA': ['64.97', '28', '20'], 'Hartford-East Hartford-Middletown, CT': ['64.07', '21', '127'], 'Asheville, NC': ['62.61', '38', '26'], 'Urban Honolulu, HI': ['62.49', '45', '10'], 'Lansing-East Lansing, MI': ['62.30', '34', '41'], 'Chicago-Naperville-Elgin, IL-IN-WI': ['62.09', '31', '58'], 'Eugene-Springfield, OR': ['61.97', '43', '18'], 'Pittsburgh, PA': ['61.59', '40', '38'], 'Santa Rosa-Petaluma, CA': ['61.38', '36', '63'], 'Salt Lake City, UT': ['61.15', '41', '46'], 'Richmond, VA': ['61.07', '32', '85'], 'Manchester-Nashua, NH': ['61.03', '26', '132'], 'Philadelphia-Camden-Wilmington, PA-NJ-DE-MD': ['60.96', '30', '95'], 'Rochester, NY': ['60.37', '33', '106'], 'Kansas City, MO-KS': ['60.13', '29', '117'], 'Omaha-Council Bluffs, NE-IA': ['60.12', '37', '90'], 'Virginia Beach-Norfolk-Newport News, VA-NC': ['60.10', '52', '23'], 'New Haven-Milford, CT': ['59.56', '39', '110'], 'Tucson, AZ': ['59.48', '49', '43'], 'St. Louis, MO-IL': ['59.00', '35', '137'], 'Columbus, OH': ['59.00', '42', '112'], 'Albuquerque, NM': ['58.80', '54', '40'], 'Nashville-Davidson--Murfreesboro--Franklin, TN': ['58.79', '53', '56'], 'Sacramento-Roseville-Folsom, CA': ['58.64', '56', '35'], 'Boise City, ID': ['58.50', '63', '21'], 'Spokane-Spokane Valley, WA': ['58.26', '55', '52'], 'Worcester, MA-CT': ['58.23', '50', '76'], 'Charlotte-Concord-Gastonia, NC-SC': ['58.00', '59', '29'], 'Charleston-North Charleston, SC': ['57.85', '47', '111'], 'Des Moines-West Des Moines, IA': ['57.20', '44', '124'], 'Cincinnati, OH-KY-IN': ['56.54', '62', '62'], 'Anchorage, AK': ['56.41', '48', '128'], 'Buffalo-Cheektowaga, NY': ['56.22', '46', '135'], 'Milwaukee-Waukesha, WI': ['56.20', '51', '134'], 'Orlando-Kissimmee-Sanford, FL': ['56.08', '75', '15'], 'Syracuse, NY': ['55.77', '58', '107'], 'Santa Maria-Santa Barbara, CA': ['55.38', '83', '11'], 'Reno, NV': ['55.37', '74', '25'], 'Grand Rapids-Kentwood, MI': ['55.29', '66', '84'], 'Naples-Marco Island, FL': ['55.24', '60', '120'], 'Columbia, SC': ['55.05', '67', '74'], 'Indianapolis-Carmel-Anderson, IN': ['54.89', '64', '105'], 'Springfield, MA': ['54.72', '70', '60'], 'North Port-Sarasota-Bradenton, FL': ['54.70', '57', '144'], 'Fayetteville-Springdale-Rogers, AR': ['54.54', '90', '2'], 'Dayton-Kettering, OH': ['54.29', '73', '54'], 'Phoenix-Mesa-Chandler, AZ': ['54.12', '81', '34'], 'Dallas-Fort Worth-Arlington, TX': ['54.12', '72', '55'], 'Detroit-Warren-Dearborn, MI': ['53.64', '65', '133'], 'Jacksonville, FL': ['53.38', '80', '57'], 'Palm Bay-Melbourne-Titusville, FL': ['53.34', '68', '123'], 'Akron, OH': ['53.04', '76', '89'], 'Oxnard-Thousand Oaks-Ventura, CA': ['53.03', '77', '68'], 'Miami-Fort Lauderdale-Pompano Beach, FL': ['52.94', '93', '8'], 'Little Rock-North Little Rock-Conway, AR': ['52.94', '82', '69'], 'Harrisburg-Carlisle, PA': ['52.75', '79', '82'], 'Birmingham-Hoover, AL': ['52.66', '78', '87'], 'Savannah, GA': ['52.34', '69', '130'], 'Oklahoma City, OK': ['52.15', '86', '53'], 'Tampa-St. Petersburg-Clearwater, FL': ['51.76', '89', '39'], 'Cleveland-Elyria, OH': ['51.51', '71', '140'], 'Ogden-Clearfield, UT': ['51.17', '61', '150'], 'Houston-The Woodlands-Sugar Land, TX': ['50.82', '94', '33'], 'Los Angeles-Long Beach-Anaheim, CA': ['50.70', '101', '22'], 'Knoxville, TN': ['50.61', '91', '59'], 'Providence-Warwick, RI-MA': ['50.59', '88', '91'], 'Jackson, MS': ['50.44', '85', '109'], 'Wichita, KS': ['50.27', '84', '122'], 'Louisville/Jefferson County, KY-IN': ['50.08', '87', '104'], 'Greenville-Anderson, SC': ['49.82', '99', '48'], 'Springfield, MO': ['49.69', '92', '72'], 'Peoria, IL': ['49.05', '100', '66'], 'New Orleans-Metairie, LA': ['48.92', '96', '78'], 'Montgomery, AL': ['48.84', '95', '100'], 'Allentown-Bethlehem-Easton, PA-NJ': ['48.57', '97', '94'], 'Greensboro-High Point, NC': ['48.41', '105', '37'], 'Pensacola-Ferry Pass-Brent, FL': ['48.16', '98', '103'], 'Toledo, OH': ['47.29', '104', '92'], 'Memphis, TN-MS-AR': ['46.59', '103', '114'], 'San Antonio-New Braunfels, TX': ['46.55', '115', '28'], 'Cape Coral-Fort Myers, FL': ['46.40', '107', '93'], 'Myrtle Beach-Conway-North Myrtle Beach, SC-NC': ['46.36', '111', '36'], 'Davenport-Moline-Rock Island, IA-IL': ['46.32', '102', '141'], 'Fort Wayne, IN': ['46.08', '108', '86'], 'Tulsa, OK': ['45.36', '110', '102'], 'Winston-Salem, NC': ['45.20', '120', '19'], 'Chattanooga, TN-GA': ['44.92', '112', '77'], 'Baton Rouge, LA': ['44.91', '114', '65'], 'Fayetteville, NC': ['44.88', '116', '51'], 'Salisbury, MD-DE': ['44.16', '106', '143'], 'Salem, OR': ['43.45', '121', '45'], 'Port St. Lucie, FL': ['42.87', '119', '99'], 'Killeen-Temple, TX': ['42.83', '113', '125'], 'Augusta-Richmond County, GA-SC': ['42.12', '117', '129'], 'Vallejo, CA': ['41.93', '109', '148'], 'Deltona-Daytona Beach-Ormond Beach, FL': ['41.33', '118', '138'], 'Las Vegas-Henderson-Paradise, NV': ['41.25', '126', '32'], 'Gulfport-Biloxi, MS': ['41.13', '122', '108'], 'Scranton--Wilkes-Barre, PA': ['40.42', '123', '119'], 'Shreveport-Bossier City, LA': ['40.09', '125', '101'], 'Lancaster, PA': ['39.67', '128', '73'], 'Flint, MI': ['39.49', '124', '126'], 'Canton-Massillon, OH': ['39.48', '129', '75'], 'York-Hanover, PA': ['39.10', '127', '98'], 'Reading, PA': ['37.17', '131', '113'], 'Rockford, IL': ['36.92', '130', '131'], 'Mobile, AL': ['36.88', '132', '80'], 'Lafayette, LA': ['36.17', '137', '6'], 'Youngstown-Warren-Boardman, OH-PA': ['35.22', '133', '88'], 'Huntington-Ashland, WV-KY-OH': ['34.97', '134', '64'], 'Riverside-San Bernardino-Ontario, CA': ['33.48', '136', '67'], 'Lakeland-Winter Haven, FL': ['32.21', '140', '61'], 'El Paso, TX': ['31.90', '139', '81'], 'Fresno, CA': ['30.00', '142', '42'], 'Salinas, CA': ['29.61', '141', '115'], 'Corpus Christi, TX': ['29.26', '138', '139'], 'Ocala, FL': ['28.74', '135', '149'], 'Beaumont-Port Arthur, TX': ['28.08', '143', '96'], 'Stockton, CA': ['25.84', '145', '97'], 'Hickory-Lenoir-Morganton, NC': ['24.75', '144', '145'], 'Modesto, CA': ['20.58', '146', '142'], 'Bakersfield, CA': ['16.74', '147', '136'], 'McAllen-Edinburg-Mission, TX': ['14.88', '150', '9'], 'Brownsville-Harlingen, TX': ['10.34', '149', '121'], 'Visalia, CA': ['9.09', '148', '147']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Additional Dataset 3#\n",
        "\n",
        "There weren't any inconsistencies, however we did add in Atlanta manually from a Google search because the wiki page had been updated and Atlanta was removed.We used BeautifulSoup to webscrape the html page to get the table we wanted, since there were multiple on the webpage. We then went through and got the populations corresponding to the cities we wanted.\n"
      ],
      "metadata": {
        "id": "D6D1dFtpTKTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extra_source3():\n",
        "  final_list=[]\n",
        "  final_dict={}\n",
        "  response=requests.get(\"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population_density\")\n",
        "  soup=BeautifulSoup(response.text,\"html.parser\")\n",
        "  # table_tag=soup.find(\"table\")\n",
        "  # h2_tag=soup.find(\"h2\",{\"class\": \"mw-headline\"})\n",
        "  table_tag=soup.find(\"table\",{\"class\":\"wikitable sortable\"})\n",
        "  tbody_tag=table_tag.find(\"tbody\")\n",
        "  # tr_tag=table_tag.find(\"tr\")\n",
        "  td_tag=tbody_tag.find_all(\"td\")\n",
        "\n",
        "  for td in td_tag:\n",
        "    final_list.append(td.text.strip())\n",
        "\n",
        "  for city in final_list:\n",
        "    city=city.split(\"\\n\")\n",
        "    final_dict[city[0]]=city[1:]\n",
        "  # print(final_dict)\n",
        "  new_dict={}\n",
        "  for key,value in final_dict.items():\n",
        "    if key==\"8,175,133\" or key==\"645,149\" or key==\"337,977\":\n",
        "      new_dict[key]=\"placeholder\"\n",
        "  # print(new_dict)\n",
        "  new_dict[\"8,175,133\"]=\"New York City\"\n",
        "  new_dict[\"645,149\"]=\"Boston\"\n",
        "  new_dict[\"337,977\"]=\"Los Angeles\"\n",
        "  new_dict[\"429,414\"]=\"Atlanta\"\n",
        "  print(new_dict)\n",
        "  f = open('CleanedPop.txt', \"w\")\n",
        "  f.write(str(final_dict))\n",
        "  f.close()\n",
        "\n",
        "############ Function Call ############\n",
        "extra_source3()"
      ],
      "metadata": {
        "id": "aYwemuCCSoXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b09fb008-10ff-47f0-83c0-8636fbe8ddd7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'8,175,133': 'New York City', '645,149': 'Boston', '337,977': 'Los Angeles', '429,414': 'Atlanta'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Additional Dataset 4#\n",
        "\n",
        "For this dataset, we cleaned the CSV file for the crime log from the Los Angeles Police Department.  To do this, we removed the columns we didn't want, added the column to determine if a crime was violent, and standardized the column headers to match the other datasets."
      ],
      "metadata": {
        "id": "ptWBsneEyK6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extra_source4():\n",
        "  df = pd.read_csv('Crime_Data_from_2010_to_2019.csv')\n",
        "  df = df[['DATE OCC', 'TIME OCC', 'Crm Cd Desc', 'Weapon Desc']]\n",
        "  df['DATE OCC'] = pd.to_datetime(df['DATE OCC'])\n",
        "  df['Year'] = pd.DatetimeIndex(df['DATE OCC']).year\n",
        "  df = df.drop('DATE OCC', axis=1)\n",
        "  df['Violent'] = 'Yes'\n",
        "  df.loc[df['Weapon Desc'].isnull(), 'Violent'] = 'No'\n",
        "  df = df.drop('Weapon Desc', axis=1)\n",
        "  df = df[df['Year'] == 2019]\n",
        "  df.rename(columns={'TIME OCC': 'Time', 'Crm Cd Desc': 'Crime'}, inplace=True)\n",
        "  print(df)\n",
        "  df.to_csv('LA Crime Cleaned.csv', index=False)\n",
        "\n",
        "############ Function Call ############\n",
        "extra_source4()"
      ],
      "metadata": {
        "id": "FHgwtSN6yLSe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c993aef1-59b8-4d0b-c9de-d2b434c9bc35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Time                                            Crime  Year Violent\n",
            "1392203  1500                                 VEHICLE - STOLEN  2019      No\n",
            "1835837   130                                 VEHICLE - STOLEN  2019      No\n",
            "1836035  1800                                 VEHICLE - STOLEN  2019      No\n",
            "1836118   100                                    THEFT, PERSON  2019      No\n",
            "1836286  1800                                    BIKE - STOLEN  2019      No\n",
            "...       ...                                              ...   ...     ...\n",
            "2119792   840          CHILD ABUSE (PHYSICAL) - SIMPLE ASSAULT  2019     Yes\n",
            "2119793   400                                            ARSON  2019     Yes\n",
            "2119794  2100           CRIMINAL THREATS - NO WEAPON DISPLAYED  2019     Yes\n",
            "2119795  1800  THEFT FROM MOTOR VEHICLE - PETTY ($950 & UNDER)  2019      No\n",
            "2119796  1615                            BURGLARY FROM VEHICLE  2019      No\n",
            "\n",
            "[218088 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inconsistency 3#\n",
        "\n",
        "In our additional dataset for the LAPD Crime Logs, the DATE OCC column, which we are using to determine if the crime is in 2019 so we can be consistent with only using data from 2019, was a type object with format \"02/20/2010 12:00:00AM\".  This does not work for our formatting of trying to get only the year as an integer.  To fix this inconsistency, we changed the column to a datetime type and then used pd.DatetimeIndex(df['DATE OCC']).year to get just the year as an integer in its own column to best fit our needs of cleaning to the data so we only have data from 2019.  There is an additional inconsistency in this dataset with the Weapon Desc column.  A lot of crimes do not use weapons so instead the values are listed as NaN.  We can actually use this to our advantage when determining if the crime is violent or not.  The crime is nonviolent if the Weapon Desc is NaN and violent if a weapon is listed."
      ],
      "metadata": {
        "id": "QKCfI9_YzDma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Additional Dataset 5#\n",
        "\n",
        "For this dataset, we did not really do any data cleaning but we wanted to include it to have the files ready for Phase III.  We manually took the the data from the images on the website and created a dictionary for it to output as a text file with the rest of the dictionaries for our project."
      ],
      "metadata": {
        "id": "_T2OFfnPzKMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extra_source5():\n",
        "  drug_use = {\"Atlanta\":{'Weed':60.5, 'Cocaine':15.3, 'Heroin':2.5, 'Meth':5.4},\\\n",
        "                \"New York City\":{'Weed':57.8, 'Cocaine':17.9, 'Heroin':2.0, 'Meth':9.8},\\\n",
        "                'Boston':{'Weed':61.3, 'Cocaine':18.6, 'Heroin':2.5, 'Meth':10.4},\\\n",
        "                'LA':{'Weed':60.1, 'Cocaine':20.7, 'Heroin':1.6, 'Meth':13.3}}\n",
        "  print(drug_use)\n",
        "  f = open('Drug Use Data.txt', \"w\")\n",
        "  f.write(str(drug_use))\n",
        "  f.close()\n",
        "\n",
        "############ Function Call ############\n",
        "extra_source5()"
      ],
      "metadata": {
        "id": "tAgbnyw5zKga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c324d4eb-de5e-4a4c-8d2f-c8ba65c4519d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Atlanta': {'Weed': 60.5, 'Cocaine': 15.3, 'Heroin': 2.5, 'Meth': 5.4}, 'New York City': {'Weed': 57.8, 'Cocaine': 17.9, 'Heroin': 2.0, 'Meth': 9.8}, 'Boston': {'Weed': 61.3, 'Cocaine': 18.6, 'Heroin': 2.5, 'Meth': 10.4}, 'LA': {'Weed': 60.1, 'Cocaine': 20.7, 'Heroin': 1.6, 'Meth': 13.3}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Additional Dataset 6#\n",
        "\n",
        "For this dataset, we cleaned the CSV file for the crime log for the Boston Police Department.  To do this, we removed the columns we didn't want and then added the column for whether or not the crime was violent.  We renamed the columns to match the rest of our datasets."
      ],
      "metadata": {
        "id": "onylW-eizxqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extra_source6():\n",
        "  df = pd.read_csv('tmp9mkqyv6b.csv')\n",
        "  df = df[['OFFENSE_DESCRIPTION', 'OCCURRED_ON_DATE', 'YEAR']]\n",
        "  df[['Date', 'Time']] = df['OCCURRED_ON_DATE'].str.split(' ', expand=True)\n",
        "  df = df.drop(['Date', 'OCCURRED_ON_DATE'], axis=1)\n",
        "  df = df[df['YEAR'] == 2019]\n",
        "  df[['Hour','Minute','Sec']] = df['Time'].str.split(':', expand=True)\n",
        "  df['Time'] = df['Hour'] + df['Minute']\n",
        "  df = df.drop(['Hour','Minute','Sec'], axis=1)\n",
        "  df['Violent'] = 'No'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'M/V - LEAVING SCENE - PROPERTY DAMAGE', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'ASSAULT SIMPLE - BATTERY', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'ASSAULT - SIMPLE', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'ASSAULT AGGRAVATED', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'ASSAULT - AGGRAVATED - BATTERY', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'BURGLARY - RESIDENTIAL - FORCE', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'M/V - LEAVING SCENE - PERSONAL INJURY', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'ASSAULT SIMPLE - BATTERYM/V ACCIDENT INVOLVING PEDESTRIAN - INJURY', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'BALLISTICS EVIDENCE/FOUND', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'WEAPON - FIREARM - CARRYING / POSSESSING, ETC', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'M/V ACCIDENT - INVOLVING PEDESTRIAN - INJURY', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'M/V ACCIDENT - INVOLVING BICYCLE - INJURY', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'BURGLARY - COMMERCIAL - FORCE', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'WEAPON VIOLATION - CARRY/ POSSESSING/ SALE/ TRAFFICKING/ OTHER', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'DISTURBING THE PEACE', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'M/V ACCIDENT - INVOLVING BICYCLE - INJURY', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'DISTURBING THE PEACE/ DISORDERLY CONDUCT/ GATHERING CAUSING ANNOYANCE/ NOISY PAR', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'DEMONSTRATIONS/RIOT', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'BREAKING AND ENTERING (B&E) MOTOR VEHICLE', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'Migrated Report - Assault/Assault & Battery'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'INTIMIDATING WITNESS', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'BURGLARY - OTHER - FORCE', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'Migrated Report - Burglary/Breaking and Entering', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'MURDER, NON-NEGLIGIENT MANSLAUGHTER', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'BREAKING AND ENTERING (B&E) MOTOR VEHICLE (NO PROPERTY STOLEN)', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'ROBBERY - HOME INVASION', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'ANIMAL ABUSE', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'INJURY BICYCLE NO M/V INVOLVED', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'Migrated Report - Death Investigation', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'ROBBERY - BANK', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'WEAPON - FIREARM - OTHER VIOLATION', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'WEAPON - OTHER - OTHER VIOLATION', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'ARSON', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'CRIMINAL HARASSMENT', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'Migrated Report - Weapons Violation', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'Migrated Report - Criminal Homicide', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'KIDNAPPING/CUSTODIAL KIDNAPPING/ ABDUCTION', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'EXPLOSIVES - POSSESSION OR USE', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'HOME INVASION', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'Migrated Report - Affray/Disturbing the Peace/Disorderly Conduct', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'FIREARM/WEAPON - ACCIDENTAL INJURY / DEATH', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'BIOLOGICAL THREATS', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'MANSLAUGHTER - VEHICLE - NEGLIGENCE', 'Violent'] = 'Yes'\n",
        "  df.loc[df['OFFENSE_DESCRIPTION'] == 'THREATS TO DO BODILY HARM', 'Violent'] = 'Yes'\n",
        "  df = df[df['OFFENSE_DESCRIPTION'] != 'Yes']\n",
        "  df.rename(columns={'OFFENSE_DESCRIPTION': 'Crime', 'YEAR':'Year'}, inplace=True)\n",
        "  print(df)\n",
        "  df.to_csv('Boston Crime Cleaned.csv', index=False)\n",
        "\n",
        "############ Function Call ############\n",
        "extra_source6()"
      ],
      "metadata": {
        "id": "SVVyKIN2zyBD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fc0276f-9854-411d-8d08-1b6bbb153b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:61: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   Crime  Year  Time Violent\n",
            "0                              THREATS TO DO BODILY HARM  2019  1200     Yes\n",
            "1                                     INVESTIGATE PERSON  2019  1630      No\n",
            "2                            VAL - VIOLATION OF AUTO LAW  2019  2100      No\n",
            "3                               PROPERTY - LOST/ MISSING  2019  1330      No\n",
            "4                                         VERBAL DISPUTE  2019  0750      No\n",
            "...                                                  ...   ...   ...     ...\n",
            "87178             Migrated Report - Investigate Property  2019  0120      No\n",
            "87179  Migrated Report - Aggravated Assault/Aggravate...  2019  0130      No\n",
            "87181                   Migrated Report - Other Part III  2019  0221      No\n",
            "87182                   Migrated Report - Other Part III  2019  0427      No\n",
            "87183             Migrated Report - Investigate Property  2019  0639      No\n",
            "\n",
            "[87145 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inconsistency 4#\n",
        "\n",
        "Similar to LAPD Crime Logs, the Boston Crime Report CSV file had another inconsistent formatting for the date/time.  To fix this, we split the 'OCCURRED_ON_DATE' column into the Date and Time columns.  To get the time column in consistent formatting with our other CSV files of crime logs from other cities, we had to split the time column and then recombine the hours and minutes to get a 3 or 4 digit number that represented the time at which the crime occured.  We were able to use the Year column to get the data from 2019."
      ],
      "metadata": {
        "id": "zOUwA8Om0O5l"
      }
    }
  ]
}